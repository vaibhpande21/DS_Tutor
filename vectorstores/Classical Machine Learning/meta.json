[
  {
    "text": "1 In this chapter • what is machine learning • is machine learning hard (spoiler: no) • what do we learn in this book • what is artificial intelligence, and how does it differ from machine learning • how do humans think, and how can we inject those ideas into a machine • some basic machine learning examples in real lifeWhat is machine learning? It is common sense, except done by a computer 1 Blue? Oh my god, it has learned!!blue, blue, blue, blue, blue, blue.Hit enter for machine to learn.",
    "source": "chapter_11_1 What is machine learning_ It is common sense_ except done by a computer.pdf::chunk0"
  },
  {
    "text": "147 In this chapter • the difference between hard assignments and soft assignments in classification models • the sigmoid function, a continuous activation function • discrete perceptrons vs. continuous perceptrons, also called logistic classifiers • the logistic regression algorithm for classifying data • coding the logistic regression algorithm in Python • using the logistic classifier in Turi Create to analyze the sentiment of movie reviews • using the softmax function to build classifiers for more than two classesA continuous approach to splitting points: Logistic classifiers 6 148 Chapter 6 I A continuous approach to splitting points: Logistic classifiers the probability that i win the lottery is 90%. i think you're being a little optimistic.you think? yes.ok. you're right. the probability that i win the lottery is 89%. sigh ...we'll get there. In the previous chapter, we built a classifier that determined if a sentence was happy or sad. But as we can imagine, some sentences are happier than others. For example, the sentence \"I'm good\" and the sentence \"Today was the most wonderful day in my life!\" are both happy, yet the second is much happier than the first. Wouldn't it be nice to have a classifier that not only",
    "source": "chapter_124_6 A continuous approach to splitting points_ Logistic classifiers.pdf::chunk0"
  },
  {
    "text": "predicts if sentences are happy or sad but that gives a rating for how happy sentences are—say, a classifier that tells us that the first sentence is 60% happy and the second one is 95% happy? In this chapter, we define the logistic classifier , which does precisely that. This classifier assigns a score from 0 to 1 to each sentence, in a way that the happier a sentence is, the higher the score it receives. In a nutshell, a logistic classifier is a type of model that works just like a perceptron classifier, except instead of returning a yes or no answer, it returns a number between 0 and 1. In this case, the goal is to assign scores close to 0 to the saddest sentences, scores close to 1 to the happiest sentences, and scores close to 0.5 to neutral sentences. This threshold of 0.5 is common in practice, though arbitrary. In chapter 7, we'll see how to adjust it to optimize our model, but for this chapter we use 0.5. This chapter relies on chapter 5, because the algorithms we develop here are similar, aside from some technical differences. Making sure you understand chapter 5 well will",
    "source": "chapter_124_6 A continuous approach to splitting points_ Logistic classifiers.pdf::chunk1"
  },
  {
    "text": "help you understand the material in this chapter. In chapter 5, we described the perceptron algorithm using an error function that tells us how good a perceptron classifier is and an iterative step that turns a classifier into a slightly better classifier. In this chapter, we learn the logistic regression algorithm, which works in a similar way. The main differences follow: • The step function is replaced by a new activation function, which returns values between 0 and 1. • The perceptron error function is replaced by a new error function, which is based on a probability calculation. • The perceptron trick is replaced by a new trick, which improves the classifier based on this new error function.",
    "source": "chapter_124_6 A continuous approach to splitting points_ Logistic classifiers.pdf::chunk2"
  },
  {
    "text": "177 In this chapter • types of errors a model can make: false positives and false negatives • putting these errors in a table: the confusion matrix • what are accuracy, recall, precision, F-score, sensitivity, and specificity, and how are they used to evaluate models • what is the ROC curve, and how does it keep track of sensitivity and specificity at the same timeHow do you measure classification models? Accuracy and its friends 7 it says healthy. . . but you never examined me! are you just telling everyone they're healthy? how accurate!i am! and so far I've been correct 99% of the time! here is your diagnosis.I've developed a cutting edge covid−19 test!",
    "source": "chapter_140_7 How do you measure classification models_ Accuracy and its friends.pdf::chunk0"
  },
  {
    "text": "205 In this chapter • what is Bayes theorem • dependent and independent events • the prior and posterior probabilities • calculating conditional probabilities based on events • using the naive Bayes model to predict whether an email is spam or ham, based on the words in the email • coding the naive Bayes algorithm in PythonUsing probability to its maximum: The naive Bayes model 8 your argument is bayes−less. Why?i always bring my rubber duckie to airplanes.Because the probability of an airplane falling is low. however, the probability of an airplane falling and also having a rubber duckie inside is much lower! 206 Chapter 8 I Using probability to its maximum: The naive Bayes model Naive Bayes is an important machine learning model used for classification. The naive Bayes model is a purely probabilistic model, which means the prediction is a number between 0 and 1, indicating the probability that a label is positive. The main component of the naive Bayes model is Bayes' theorem. Bayes' theorem plays a fundamental role in probability and statistics, because it helps calculate probabilities. It is based on the premise that the more information we gather about an event, the better estimate of",
    "source": "chapter_159_8 Using probability to its maximum_  The naive Bayes model.pdf::chunk0"
  },
  {
    "text": "the probability we can make. For example, let's say we want to find the probability that it will snow today. If we have no information of where we are and what time of the year it is, we can only come up with a vague estimate. However, if we are given information, we can make a better estimate of the probability. Imagine that I tell you that I am thinking of a type of animal, and I would like you to guess it. What is the probability that the animal I'm thinking of is a dog? Given that you don't know any information, the probability is quite small. However, if I tell you that the animal I'm thinking of is a house pet, the probability increases quite a bit. However, if I now tell you that the animal I'm thinking of has wings, the probability is now zero. Each time I tell you a new piece of information, your estimate for the probability that it's a dog becomes more and more accurate. Bayes' theorem is a way to formalize this type of logic and put it into formulas. More specifically, Bayes' theorem answers the question, \"What is the probability of",
    "source": "chapter_159_8 Using probability to its maximum_  The naive Bayes model.pdf::chunk1"
  },
  {
    "text": "Y given that X occurred?\" which is called a conditional probability . As you can imagine, answering this type of question is useful in machine learning, because if we can answer the question, \"What is the probability that the label is positive given the features ?\" we have a classification model. For example, we can build a sentiment analysis model (just like we did in chapter 6) by answering the question, \"What is the probability that this sentence is happy given the words that it contains ?\" However, when we have too many features (in this case, words), the computation of the probability using Bayes' theorem gets very complicated. This is where the naive Bayes algorithm comes to our rescue. The naive Bayes algorithm uses a slick simplification of this calculation to help us build our desired classification model, called the naive Bayes model . It's called naive Bayes because to simplify the calculations, we make a slightly naive assumption that is not necessarily true. However, this assumption helps us come up with a good estimate of the probability. In this chapter, we see Bayes theorem used with some real-life examples. We start by studying an interesting and slightly surprising",
    "source": "chapter_159_8 Using probability to its maximum_  The naive Bayes model.pdf::chunk2"
  },
  {
    "text": "medical example. Then we dive deep into the naive Bayes model by applying it to a common problem in machine learning: spam classification. We finalize by coding the algorithm in Python and using it to make predictions in a real spam email dataset. All the code for this chapter is available at this GitHub repository: https://github.com/ luisguiserrano/manning/tree/master/Chapter_8_Naive_Bayes.",
    "source": "chapter_159_8 Using probability to its maximum_  The naive Bayes model.pdf::chunk3"
  },
  {
    "text": "233 In this chapter • what is a decision tree • using decision trees for classification and regression • building an app-recommendation system using users' information • accuracy, Gini index, and entropy, and their role in building decision trees • using Scikit-Learn to train a decision tree on a university admissions datasetSplitting data by asking questions: Decision trees 9 i have a problem with my computer.does it start? is it plugged in? does smoke come out of the back?no yes. yes.i conclude that you have a problem with your computer. . . . 234 Chapter 9 I Splitting data by asking questions: Decision trees In this chapter, we cover decision trees. Decision trees are powerful classification and regression models, which also give us a great deal of information about our dataset. Just like the previous models we've learned in this book, decision trees are trained with labeled data, where the labels that we want to predict can be classes (for classification) or values (for regression). For most of this chapter, we focus on decision trees for classification, but near the end of the chapter, we describe decision trees for regression. However, the structure and training process of both types of",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk0"
  },
  {
    "text": "tree is similar. In this chapter, we develop several use cases, including an app-recommendation system and a model for predicting admissions at a university. Decision trees follow an intuitive process to make predictions—one that very much resembles human reasoning. Consider the following scenario: we want to decide whether we should wear a jacket today. What does the decision process look like? We may look outside and check if it's raining. If it's raining, then we definitely wear a jacket. If it's not, then maybe we check the temperature. If it is hot, then we don't wear a jacket, but if it is cold, then we wear a jacket. In figure 9.1, we can see a graph of this decision process, where the decisions are made by traversing the tree from top to bottom. Raining? Hot?Yes No Don't wear a jacket.Wear a jacket. Wear a jacket.Yes No Figure 9.1 A decision tree used to decide whether we want to wear a jacket or not on a given day. We make the decision by traversing the tree down and taking the branch corresponding to each correct answer. Our decision process looks like a tree, except it is upside down. The tree is",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk1"
  },
  {
    "text": "formed of vertices, called nodes , and edges. On the very top, we can see the root node , from which two branches emanate. Each of the nodes has either two or zero branches (edges) emanating from them, and for this reason, we call it a binary tree . The nodes that have two branches emanating from them are called decision nodes , and the nodes with no branches emanating from them are called leaf nodes , or leaves . This arrangement of nodes, leaves, and edges is what we call a decision tree. Trees are natural objects in computer science, because computers break every process into a sequence of binary operations. The simplest possible decision tree, called a decision stump , is formed by a single decision node (the root node) and two leaves. This represents a single yes-or-no question, based on which we immediately make a decision. Splitting data by asking questions: Decision trees 235 The depth of a decision tree is the number of levels underneath the root node. Another way to measure it is by the length of the longest path from the root node to a leaf, where a path is measured by the number",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk2"
  },
  {
    "text": "of edges it contains. The tree in figure 9.1 has a depth of 2. A decision stump has a depth of 1. Here is a summary of the definitions we've learned so far: decision tree A machine learning model based on yes-or-no questions and represented by a binary tree. The tree has a root node, decision nodes, leaf nodes, and branches. root node The topmost node of the tree. It contains the first yes-or-no question. For convenience, we refer to it as the root. decision node Each yes-or-no question in our model is represented by a decision node, with two branches emanating from it (one for the \"yes\" answer, and one for the \"no\" answer). leaf node A node that has no branches emanating from it. These represent the decisions we make after traversing the tree. For convenience, we refer to them as leaves . branch The two edges emanating from each decision node, corresponding to the \"yes\" and \"no\" answers to the question in the node. In this chapter, by convention, the branch to the left corresponds to \"yes\" and the branch to the right to \"no.\" depth The number of levels in the decision tree. Alternatively, it is",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk3"
  },
  {
    "text": "the number of branches on the longest path from the root node to a leaf node. Throughout this chapter, nodes are drawn as rectangles with rounded edges, the answers in the branches as diamonds, and leaves as ovals. Figure 9.2 shows how a decision tree looks in general. Raining? Hot?Root node Decision node Leaf LeafLeafYes No Yes No Don't wear a jacket.Wear a jacket. Wear a jacket.Branch Figure 9.2 A regular decision tree with a root node, decision nodes, branches, and leaves. Note that each decision node contains a yes-or-no question. From each possible answer, one branch emanates, which can lead to another decision node or a leaf. This tree has a depth of 2, because the longest path from a leaf to the root goes through two branches. 236 Chapter 9 I Splitting data by asking questions: Decision trees How did we build this tree? Why were those the questions we asked? We could have also checked if it was Monday, if we saw a red car outside, or if we were hungry, and built the following decision tree: Monday? Hungry? Red car?Yes No Don't wear a jacket.Wear a jacket.Don't wear a jacket.Wear a jacket.Yes No Yes No Figure",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk4"
  },
  {
    "text": "9.3 A second (maybe not as good) decision tree we could use to decide whether we want to wear a jacket on a given day Which tree do we think is better when it comes to deciding whether or not to wear a jacket: tree 1 (figure 9.1) or tree 2 (figure 9.3)? Well, as humans, we have enough experience to figure out that tree 1 is much better than tree 2 for this decision. How would a computer know? Computers don't have experience per se, but they have something similar, which is data. If we wanted to think like a computer, we could just go over all possible trees, try each one of them for some time—say, one year —and compare how well they did by counting how many times we made the right decision using each tree. We'd imagine that if we use tree 1, we were correct most days, whereas if we used tree 2, we may have ended up freezing on a cold day without a jacket or wearing a jacket on an extremely hot day. All a computer has to do is go over all trees, collect data, and find which one is the best",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk5"
  },
  {
    "text": "one, right? Almost! Unfortunately, even for a computer, searching over all the possible trees to find the most effective one would take a really long time. But luckily, we have algorithms that make this search much faster, and thus, we can use decision trees for many wonderful applications, including spam detection, sentiment analysis, and medical diagnosis. In this chapter, we'll go over an algorithm for constructing good decision trees quickly. In a nutshell, we build the tree one node at a time, starting from the top. To pick the right question corresponding to each node, we go over all the possible questions we can ask and pick the one that is right the highest number of times. The process goes as follows: Splitting data by asking questions: Decision trees 237 Picking a good first question We need to pick a good first question for the root of our tree. What would be a good question that helps us decide whether to wear a jacket on a given day? Initially, it can be anything. Let's say we come up with five candidates for our first question: 1. Is it raining? 2. Is it cold outside? 3. Am I hungry? 4. Is",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk6"
  },
  {
    "text": "there a red car outside? 5. Is it Monday? Out of these five questions, which one seems like the best one to help us decide whether we should wear a jacket? Our intuition says that the last three questions are useless to help us decide. Let's say that from experience, we've noticed that among the first two, the first one is more useful. We use that question to start building our tree. So far, we have a simple decision tree, or a decision stump, consisting of that single question, as illustrated in Figure 9.4. Raining? Yes No Wear a jacket.Don't wear a jacket. Figure 9.4 A simple decision tree (decision stump) that consists of only the question, \"Is it raining?\" If the answer is yes, the decision we make is to wear a jacket. Can we do better? Imagine that we start noticing that when it rains, wearing a jacket is always the correct decision. However, there are days on which it doesn't rain, and not wearing a jacket is not the correct decision. This is where question 2 comes to our rescue. We use that question to help us in the following way: after we check that it is",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk7"
  },
  {
    "text": "not raining, then we check the temperature, and if it is cold, we decide to wear a jacket. This turns the left leaf of the tree into a node, with two leaves emanating from it, as shown in figure 9.5. 238 Chapter 9 I Splitting data by asking questions: Decision trees Raining? Hot?Yes No Wear a jacket. Don't wear a jacket.Wear a jacket.Yes No Figure 9.5 A slightly more complicated decision tree than the one in figure 9.4, where we have picked one leaf and split it into two further leaves. This is the same tree as in figure 9.1. Now we have our decision tree. Can we do better? Maybe we can if we add more nodes and leaves to our tree. But for now, this one works very well. In this example, we made our decisions using our intuition and our experience. In this chapter, we learn an algorithm that builds these trees solely based on data. Many questions may arise in your head, such as the following: 1. How exactly do you decide which is the best possible question to ask? 2. Does the process of always picking the best possible question actually get us to build",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk8"
  },
  {
    "text": "the best decision tree? 3. Why don't we instead build all the possible decision trees and pick the best one from there? 4. Will we code this algorithm? 5. Where can we find decision trees in real life? 6. We can see how decision trees work for classification, but how do they work for regression? This chapter answers all of these questions, but here are some quick answers: 1. How exactly do you decide which is the best possible question to ask? We have several ways to do this. The simplest one is using accuracy, which means: which question helps me be correct more often? However, in this chapter, we also learn other methods, such as Gini index or entropy. 2. Does the process of always picking the best possible question actually get us to build the best decision tree? Actually, this process does not guarantee that we get the best possible tree. This is what we call a greedy algorithm . Greedy algorithms work as follows: at every point, the Splitting data by asking questions: Decision trees 239 algorithm makes the best possible available move. They tend to work well, but it's not always the case that making the",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk9"
  },
  {
    "text": "best possible move at each timestep gets you to the best overall outcome. There may be times in which asking a weaker question groups our data in a way that we end up with a better tree at the end of the day. However, the algorithms for building decision trees tend to work very well and very quickly, so we'll live with this. Look at the algorithms that we see in this chapter, and try to figure out ways to improve them by removing the greedy property! 3. Why don't we instead build all the possible decision trees and pick the best one from there? The number of possible decision trees is very large, especially if our dataset has many features. Going through all of them would be very slow. Here, finding each node requires only a linear search across the features and not across all the possible trees, which makes it much faster. 4. Will we code this algorithm? This algorithm can be coded by hand. However, we'll see that because it is recursive, the coding can get a bit tedious. Thus, we'll use a useful package called Scikit-Learn to build decision trees with real data. 5. Where can",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk10"
  },
  {
    "text": "we find decision trees in real life? In many places! They are used extensively in machine learning, not only because they work very well but also because they give us a lot of information on our data. Some places in which decision trees are used are in recommendation systems (to recommend videos, movies, apps, products to buy, etc.), in spam classification (to decide whether or not an email is spam), in sentiment analysis (to decide whether a sentence is happy or sad), and in biology (to decide whether or not a patient is sick or to help identify certain hierarchies in species or in types of genomes). 6. We can see how decision trees work for classification, but how do they work for regression? A regression decision tree looks exactly like a classification decision tree, except for the leaves. In a classification decision tree, the leaves have classes, such as yes and no. In a regression decision tree, the leaves have values, such as 4, 8.2, or –199. The prediction our model makes is given by the leaf at which we arrived when traversing the tree in a downward fashion. The first use case that we'll study in this chapter",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk11"
  },
  {
    "text": "is a popular application in machine learning, and one of my favorites: recommendation systems. The code for this chapter is available in this GitHub repository: https://github.com/luisguiserrano/ manning/tree/master/Chapter_9_Decision_Trees.",
    "source": "chapter_176_9 Splitting data by asking questions_ Decision trees.pdf::chunk12"
  },
  {
    "text": "277 In this chapter • what is a neural network • the architecture of a neural network: nodes, layers, depth, and activation functions • training neural networks using backpropagation • potential problems in training neural networks, such as the vanishing gradient problem and overfitting • techniques to improve neural network training, such as regularization and dropout • using Keras to train neural networks for sentiment analysis and image classification • using neural networks as regression modelsCombining building blocks to gain more power: Neural networks 10 278 Chapter 10 I Combining building blocks to gain more power: Neural networks Carl has been tr i y hard a ning reall these days!Impressive! In this chapter, we learn neural networks , also called multilayer perceptrons . Neural networks are one of the most popular (if not the most popular) machine learning models out there. They are so useful that the field has its own name: deep learning . Deep learning has numerous applications in the most cutting-edge areas of machine learning, including image recognition, natural language processing, medicine, and self-driving cars. Neural networks are meant to, in a broad sense of the word, mimic how the human brain operates. They can be very",
    "source": "chapter_196_10 Combining building blocks to gain more power_ Neural networks.pdf::chunk0"
  },
  {
    "text": "complex, as figure 10.1 shows. Figure 10.1 A neural network. It may look complicated, but in the next few pages, we will demystify this image. The neural network in figure 10.1 may look scary with lots of nodes, edges, and so on. However, we can understand neural networks in much simpler ways. One way to see them is as a collection of perceptrons (which we learned in chapters 5 and 6). I like to see neural networks as compositions of linear classifiers that give rise to nonlinear classifiers. In low dimensions, the linear classifiers would look like lines or planes, and the nonlinear classifiers would look like complicated curves or surfaces. In this chapter, we discuss the intuition behind neural networks and the details about how they work, and we also code neural networks and use them for several applications such as image recognition.",
    "source": "chapter_196_10 Combining building blocks to gain more power_ Neural networks.pdf::chunk1"
  },
  {
    "text": "315 In this chapter • what a support vector machine is • which of the linear classifiers for a dataset has the best boundary • using the kernel method to build nonlinear classifiers • coding support vector machines and the kernel method in Scikit-LearnFinding boundaries with style: Support vector machines and the kernel method 11 Experts recommend the kernel method when attempting to separate chicken datasets. 316 Chapter 11 I Finding boundaries with style: Support vector machines and the kernel method In this chapter, we discuss a powerful classification model called the support vector machine (SVM for short). An SVM is similar to a perceptron, in that it separates a dataset with two classes using a linear boundary. However, the SVM aims to find the linear boundary that is located as far as possible from the points in the dataset. We also cover the kernel method, which is useful when used in conjunction with an SVM, and it can help classify datasets using highly nonlinear boundaries. In chapter 5, we learned about linear classifiers, or perceptrons. With two-dimensional data, these are defined by a line that separates a dataset consisting of points with two labels. However, we may have noticed",
    "source": "chapter_223_11 Finding boundaries with style_ Support vector machines and the kernel method.pdf::chunk0"
  },
  {
    "text": "that many different lines can separate a dataset, and this raises the following question: how do we know which is the best line? In figure 11.1, we can see three different linear classifiers that separate this dataset. Which one do you prefer, classifier 1, 2, or 3? Classifier 2 Classifier 1 Classifier 3 Figure 11.1 Three classifiers that classify our data set correctly. Which should we prefer, classifier 1, 2, or 3? If you said classifier 2, we agree. All three lines separate the dataset well, but the second line is better placed. The first and third lines are very close to some of the points, whereas the second line is far from all the points. If we were to wiggle the three lines around a little bit, the first and the third may go over some of the points, misclassifying some of them in the process, whereas the second one will still classify them all correctly. Thus, classifier 2 is more robust than classifiers 1 and 3. This is where support vector machines come into play. An SVM classifier uses two parallel lines instead of one line. The goal of the SVM is twofold; it tries to classify the",
    "source": "chapter_223_11 Finding boundaries with style_ Support vector machines and the kernel method.pdf::chunk1"
  },
  {
    "text": "data correctly and also tries to space the lines as much as possible. In figure 11.2, we can see the two parallel lines for the three classifiers, together with their middle line for reference. The two external (dotted) lines in classifier 2 are the farthest from each other, which makes this classifier the best one. Classifier 2 Classifier 1C lassifier 3 Figure 11.2 We draw our classifier as two parallel lines, as far apart from each other as possible. We can see that classifier 2 is the one where the parallel lines are the farthest away from each other. This means that the middle line in classifier 2 is the one best located between the points. Finding boundaries with style: Support vector machines and the kernel method 317 We may want to visualize an SVM as the line in the middle that tries to stay as far as possible from the points. We can also imagine it as the two external parallel lines trying to stay as far away from each other as possible. In this chapter, we'll use both visualizations at different times, because each of them is useful in certain situations. How do we build such a classifier?",
    "source": "chapter_223_11 Finding boundaries with style_ Support vector machines and the kernel method.pdf::chunk2"
  },
  {
    "text": "We can do this in a similar way as before, with a slightly different error function and a slightly different iterative step. note In this chapter, all the classifiers are discrete, namely, their output is 0 or 1. Sometimes they are described by their prediction yˆ = step(f(x)), and other times by their boundary equation f(x) = 0, namely, the graph of the function that attempts to separate our data points into two classes. For example, the perceptron that makes the prediction yˆ = step(3x1 + 4x2 – 1) sometimes is described only by the linear equation 3 x1 + 4x2 – 1 = 0. For some classifiers in this chapter, especially those in the section \"Training SVMs with nonlinear boundaries: The kernel method,\" the boundary equation will not necessarily be a linear function. In this chapter, we see this theory mostly on datasets of one and two dimensions (points on a line or on the plane). However, support vector machines work equally well in datasets of higher dimensions. The linear boundaries in one dimension are points and in two dimensions are lines. Likewise, the linear boundaries in three dimensions are planes, and in higher dimensions, they are hyperplanes of",
    "source": "chapter_223_11 Finding boundaries with style_ Support vector machines and the kernel method.pdf::chunk3"
  },
  {
    "text": "one dimension less than the space in which the points live. In each of these cases, we try to find the boundary that is the farthest from the points. In figure 11.3, you can see examples of boundaries for one, two, and three dimensions. Two dimensions One dimension Three dimensions Figure 11.3 Linear boundaries for datasets in one, two, and three dimensions. In one dimension, the boundary is formed by two points, in two dimensions by two lines, and in three dimensions by two planes. In each of the cases, we try to separate these two as much as possible. The middle boundary (point, line, or plane) is illustrated for clarity. All the code for this chapter is in this GitHub repository: https://github.com/luisguiserrano/ manning/tree/master/Chapter_11_Support_Vector_Machines.",
    "source": "chapter_223_11 Finding boundaries with style_ Support vector machines and the kernel method.pdf::chunk4"
  },
  {
    "text": "351 In this chapter • what ensemble learning is, and how it is used to combine weak classifiers into a stronger one • using bagging to combine classifiers in a random way • using boosting to combine classifiers in a cleverer way • some of the most popular ensemble methods: random forests, AdaBoost, gradient boosting, and XGBoostCombining models to maximize results: Ensemble learning 12 Oh no, they've learned ensemble methods!!",
    "source": "chapter_239_12 Combining models to maximize results_ Ensemble learning.pdf::chunk0"
  },
  {
    "text": "387 In this chapter • cleaning up and preprocessing data to make it readable by our model • using Scikit-Learn to train and evaluate several models • using grid search to select good hyperparameters for our model • using k-fold cross-validation to be able to use our data for training and validation simultaneouslyPutting it all in practice: A real-life example of data engineering and machine learning13 I'm not concerned. My model says I will survive!",
    "source": "chapter_258_13 Putting it all in practice_ A real-life example of data engineering and machine learning.pdf::chunk0"
  },
  {
    "text": "15 In this chapter • three different types of machine learning: supervised, unsupervised, and reinforcement learning • the difference between labeled and unlabeled data • the difference between regression and classification, and how they are usedTypes of machine learning 2 What is our salary increase for next year? classifaction help deskYes!sigh! where can i find the regression help desk? No! classifaction help deskclassifaction help desk 16 Chapter 2 I Types of machine learning As we learned in chapter 1, machine learning is common sense for a computer. Machine learning roughly mimics the process by which humans make decisions based on experience, by making decisions based on previous data. Naturally, programming computers to mimic the human thinking process is challenging, because computers are engineered to store and process numbers, not make decisions. This is the task that machine learning aims to tackle. Machine learning is divided into several branches, depending on the type of decision to be made. In this chapter, we overview some of the most important among these branches. Machine learning has applications in many fields, such as the following: • Predicting house prices based on the house's size, number of rooms, and location • Predicting today's stock market",
    "source": "chapter_26_2 Types of machine learning.pdf::chunk0"
  },
  {
    "text": "prices based on yesterday's prices and other factors of the market • Detecting spam and non-spam emails based on the words in the e-mail and the sender • Recognizing images as faces or animals, based on the pixels in the image • Processing long text documents and outputting a summary • Recommending videos or movies to a user (e.g., on YouTube or Netflix) • Building chatbots that interact with humans and answer questions • Training self-driving cars to navigate a city by themselves • Diagnosing patients as sick or healthy • Segmenting the market into similar groups based on location, acquisitive power, and interests • Playing games like chess or Go Try to imagine how we could use machine learning in each of these fields. Notice that some of these applications are different but can be solved in a similar way. For example, predicting housing prices and predicting stock prices can be done using similar techniques. Likewise, predicting whether an email is spam and predicting whether a credit card transaction is legitimate or fraudulent can also be done using similar techniques. What about grouping users of an app based on their similarity? That sounds different from predicting housing prices, but",
    "source": "chapter_26_2 Types of machine learning.pdf::chunk1"
  },
  {
    "text": "it could be done similarly to grouping newspaper articles by topic. And what about playing chess? That sounds different from all the other previous applications, but it could be like playing Go. Machine learning models are grouped into different types, according to the way they operate. The main three families of machine learning models are • supervised learning , • unsupervised learning , and • reinforcement learning .",
    "source": "chapter_26_2 Types of machine learning.pdf::chunk2"
  },
  {
    "text": "411 Appendix Solutions to the exercises A Chapter 2: Types of machine learning For the questions in this chapter, your answers don't need to match mine. If you have different ideas for models used in these applications, they might be great! I encourage you to look them up in the literature, and if they don't exist, try to implement them. Exercise 2.1 For each of the following scenarios, state if it is an example of supervised or unsupervised learning. Explain your answers. In cases of ambiguity, pick one and explain why you picked it. a. A recommendation system on a social network that recommends potential friends to a user b. A system in a news site that divides the news into topics c. The Google autocomplete feature for sentences d. A recommendation system on an online retailer that recommends to users what to buy based on their past purchasing history e. A system in a credit card company that captures fraudulent transactions Solution Depending on how you interpreted the problem and the dataset, each of these can be considered an example of supervised or unsupervised learning. It is completely OK (and encouraged!) to have different answers, as long as the",
    "source": "chapter_279_Appendix A_ Solutions to the exercises.pdf::chunk0"
  },
  {
    "text": "reasoning behind them is correct. a. This is an example of both supervised and unsupervised learning. Supervised learning: for a particular user, we can build a classification model where the label of every other user is positive if they are a potential friend and negative if they are not a potential friend. Unsupervised learning: we can cluster the users, where similar users share similar",
    "source": "chapter_279_Appendix A_ Solutions to the exercises.pdf::chunk1"
  },
  {
    "text": "449 Appendix The math behind gradient descent: Coming down a mountain using derivatives and slopesB In this appendix, we'll go over the mathematical details of gradient descent. This appendix is fairly technical, and understanding it is not required to follow the rest of the book. However, it is here to provide a sense of completeness for the readers who wish to understand the inner workings of some of the core machine learning algorithms. The mathematics knowledge required for this appendix is higher than for the rest of the book. More specifically, knowledge of vectors, derivatives, and the chain rule is required. In chapters 3, 5, 6, 10, and 11, we used gradient descent to minimize the error functions in our models. More specifically, we used gradient descent to minimize the following error functions: • Chapter 3: the absolute and square error functions in a linear regression model • Chapter 5: the perceptron error function in a perceptron model • Chapter 6: the log loss in a logistic classifier • Chapter 10: the log loss in a neural network • Chapter 11: the classification (perceptron) error and the distance (regularization) error in an SVM As we learned in chapters 3, 5,",
    "source": "chapter_325_Appendix B_ The math behind gradient descent_ Coming down a mountain  using derivatives and slopes.pdf::chunk0"
  },
  {
    "text": "6, 10, and 11, the error function measures how poorly the model is doing. Thus, finding the minimum value for this error function—or at least a really small value, even if it's not the minimum—will be instrumental in finding a good model. The analogy we used was that of descending a mountain—Mount Errorest, shown in figure B.1. The scenario is the following: You are somewhere on top of a mountain, and you'd like to get to the bottom of this mountain. It is very cloudy, so you can't see far around you. The best bet you can have is to descend from the mountain one step at a time. You ask yourself , \"If I were to take only one step, in which direction should I take it to descend the most?\" You find that direction and take that step. Then you ask the same question again, and take another step, and you repeat the process many times. It is imaginable that if you always take the one step that helps you descend the most, that you must",
    "source": "chapter_325_Appendix B_ The math behind gradient descent_ Coming down a mountain  using derivatives and slopes.pdf::chunk1"
  },
  {
    "text": "471 Appendix References C These references can also be found in https://serrano.academy/grokking-machine-learning/ . General references • GitHub repository: www.github.com/luisguiserrano/manning • YouTube videos: www.youtube.com/c/LuisSerrano • General information: https://serrano.academy • Book information: https://serrano.academy/grokking-machine-learning Courses • Udacity machine learning nanodegree: http://mng.bz/4KE5 • Coursera machine learning course: https://www.coursera.org/learn/machine-learning • Coursera machine learning specialization (University of Washington): http://mng.bz/Xryl • End-to-end machine learning: https://end-to-end-machine-learning.teachable.com/courses",
    "source": "chapter_333_Appendix C_ References.pdf::chunk0"
  },
  {
    "text": "481 index A absolute error 61 , 62, 154 absolute trick 55–56 accuracy decision trees with continuous features 260 with yes/no questions 243–244 examples of models 178–179 receiver operating characteristic (ROC) curve 189–200 AUC (area under the curve) 194–195 making decisions using 195–196 sensitivity and specificity 189–194 , 198–200 testing 403 types of errors 179–188 choosing 189 confusion matrix 182–183 false positives and false negatives 180–182 F-score 186–189 precision 185–186 recall 183–184Acharya, Mohan S. 476 activation functions 286 , 297–298 AdaBoost 360–369 building weak learners 361–363 coding in Scikit-Learn 368–369 combining weak learners into strong learner 363–366 combining classifiers 366 probability, odds, and logodds 364 AdaBoostClassifier package 368 Admissions dataset 475 AI (artificial intelligence) 3–4 , 4 Alammar, Jay 472 , 477 algorithms 7–13 clustering 23–25 examples of models that humans use 8–12 examples of models that machines use 12–13 linear regression 44–45 general linear regression algorithm 59–60 overview 56 using in dataset 58 using model to make predictions 59alien planet example classification 106 bias and y-intercept 119–120 classifiers, goal of 114 definition of perceptron classifier 117–119 general classifier 114–115 slightly more complicated planet 110–113 step function and activation functions 116 neural networks 279–292 architecture of fully connected neural network",
    "source": "chapter_389_index.pdf::chunk0"
  },
  {
    "text": "291–292 boundary of neural network 289–291 combining outputs of perceptrons into another perceptron 283–285 graphical representation of neural networks 287 graphical representation of perceptrons 285–287 using two lines to classify dataset 280–281 AND dataset 421 AND gate 420 AND operator 283 , 332 Antony, Aneeta S. 476 apply() function 227 480 Appendix C I References • Figures 2.2, 2.3, 2.4—factory: DinosoftLabs • Figures 2.5, 2.6—envelope: Freepik • Figure 2.7—house: Vectors Market • Figure 2.8—map point: Freepik • Figures 2.11, 2.12—robot, mountain: Freepik; dragon: Eucalyp; Treasure: Smashicons • Figures 3.3, 3.4, 3.5—house: Vectors Market • Figures 3.20, 3.21, 3.22—hiker, mountain range: Freepik; flag, bulb: Good Ware • Figure 4.1—Godzilla: Freepik; swatter: Smashicons; fly: Eucalyp; bazooka: photo3idea_ studio • Figure 4.6—house: Vectors Market; bandage, shingles: Freepik; titanium rod: Vitaly Gorbachev • Figures 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.10, 5.11, 5.12, 5.13, 5.16, 5.17, 5.18, 5.19, 5.20, 5.21, 5.22, 5.23, 5.24, 6.1, 6.4, 6.5, 6.6, 10.2, 10.3—happy face, sad face, neutral face: Vectors Market • Figures 5.15, 5.16—hiker: Freepik; bulb: Good Ware • Figure 7.11 (in the exercises)—various animals: surang; book: Good Ware; clock, strawberry, planet, soccer ball: Freepik; video: monkik • Figure 8.4—chef hat: Vitaly Gorbachev; wheat, bread: Freepik • Figures",
    "source": "chapter_389_index.pdf::chunk1"
  },
  {
    "text": "8.5, 8.6, 8.7, 8.8, 8.9, 8.10, 8.11, 8.12, 8.13—envelope: Smashicons • Figures 9.6, 9.7, 9.8, 9.9, 9.12, 9.13, 9.15, 9.16, 9.17—atom: mavadee; beehive: Smashicons; chess knight: Freepik • Figures 9.13, 9.17—handbag: Gregor Cresnar • Joke in chapter 10—arms with muscle: Freepik • Joke in chapter 11—corn, chicken: Freepik; chicken: monkik • Joke in chapter 12—shark, fish: Freepik • Figure 12.1—big robot: photo3idea_studio; small robots: Freepik • Joke in chapter 13—ship: Freepik",
    "source": "chapter_389_index.pdf::chunk2"
  },
  {
    "text": "35 In this chapter • what is linear regression • fitting a line through a set of data points • coding the linear regression algorithm in Python • using Turi Create to build a linear regression model to predict housing prices in a real dataset • what is polynomial regression • fitting a more complex curve to nonlinear data • discussing examples of linear regression in the real world, such as medical applications and recommender systemsDrawing a line close to our points: Linear regression 3 36 Chapter 3 I Drawing a line close to our points: Linear regression the next day i drank two cups of coffee and i managed to write two pages.i drank one cup of coffee and i managed to write one page.what do you think would happen if I drink three cups? i don't know. I'm no data scientist. In this chapter, we will learn about linear regression. Linear regression is a powerful and widely used method to estimate values, such as the price of a house, the value of a certain stock, the life expectancy of an individual, or the amount of time a user will watch a video or spend on a website. You",
    "source": "chapter_44_3 Drawing a line close to our points_ Linear regression.pdf::chunk0"
  },
  {
    "text": "may have seen linear regression before as a plethora of complicated formulas including derivatives, systems of equations, and determinants. However, we can also see linear regression in a more graphical and less formulaic way. In this chapter, to understand linear regression, all you need is the ability to visualize points and lines moving around. Let's say that we have some points that roughly look like they are forming a line, as shown in figure 3.1. Figure 3.1 Some points that roughly look like they are forming a line The goal of linear regression is to draw the line that passes as close to these points as possible. What line would you draw that passes close to those points? How about the one shown in figure 3.2? Think of the points as houses in a town, and our goal is to build a road that goes through the town. We want the line to pass as close as possible to the points because the town's inhabitants all want to live close to the road, and our goal is to please them as much as we can.",
    "source": "chapter_44_3 Drawing a line close to our points_ Linear regression.pdf::chunk1"
  },
  {
    "text": "77 In this chapter • what is underfitting and overfitting • some solutions for avoiding overfitting: testing, the model complexity graph, and regularization • calculating the complexity of the model using the L1 and L2 norms • picking the best model in terms of performance and complexityOptimizing the training process: Underfitting, overfitting, testing, and regularization4 oh no! what are you going to do?i went for lunch and when i came back my model had overfit! genius.I'm never going for lunch again. 78 Chapter 4 I Optimizing the training process: Underfitting, overfitting, testing, and regularization This chapter is different from most of the chapters in this book, because it doesn't contain a particular machine learning algorithm. Instead, it describes some potential problems that machine learning models may face and effective practical ways to solve them. Imagine that you have learned some great machine learning algorithms, and you are ready to apply them. You go to work as a data scientist, and your first task is to build a machine learning model for a dataset of customers. You build it and put it in production. However, everything goes wrong, and the model doesn't do a good job of making predictions. What happened?",
    "source": "chapter_81_4 Optimizing the training process_ Underfitting_ overfitting_ testing_ and regularization.pdf::chunk0"
  },
  {
    "text": "It turns out that this story is common, because many things can go wrong with our models. Fortunately, we have several techniques to improve them. In this chapter, I show you two problems that happen often when training models: underfitting and overfitting. I then show you some solutions to avoid underfitting and overfitting our models: testing and validation, the model complexity graph, and regularization. Let's explain underfitting and overfitting with the following analogy. Let's say that we have to study for a test. Several things could go wrong during our study process. Maybe we didn't study enough. There's no way to fix that, and we'll likely perform poorly in our test. What if we studied a lot but in the wrong way. For example, instead of focusing on learning, we decided to memorize the entire textbook word for word. Will we do well in our test? It's likely that we won't, because we simply memorized everything without learning. The best option, of course, would be to study for the exam properly and in a way that enables us to answer new questions that we haven't seen before on the topic. In machine learning, underfitting looks a lot like not having",
    "source": "chapter_81_4 Optimizing the training process_ Underfitting_ overfitting_ testing_ and regularization.pdf::chunk1"
  },
  {
    "text": "studied enough for an exam. It happens when we try to train a model that is too simple, and it is unable to learn the data. Overfitting looks a lot like memorizing the entire textbook instead of studying for the exam. It happens when we try to train a model that is too complex, and it memorizes the data instead of learning it well. A good model, one that neither underfits nor overfits, is one that looks like having studied well for the exam. This corresponds to a good model that learns the data properly and can make good predictions on new data that it hasn't seen. Another way to think of underfitting and overfitting is when we have a task in hand. We can make two mistakes. We can oversimplify the problem and come up with a solution that is too simple. We can also overcomplicate the problem and come up with a solution that is too complex. Imagine if our task is to kill Godzilla, as shown in figure 4.1, and we come to battle equipped with nothing but a fly swatter. That is an example of an oversimplification . The approach won't go well for us, because",
    "source": "chapter_81_4 Optimizing the training process_ Underfitting_ overfitting_ testing_ and regularization.pdf::chunk2"
  },
  {
    "text": "we underestimated the problem and came unprepared. This is underfitting: our dataset is complex, and we come to model it equipped with nothing but a simple model. The model will not be able to capture the complexities of the dataset. In contrast, if our task is to kill a small fly and we use a bazooka to do the job, this is an example of an overcomplication . Yes, we may kill the fly, but we'll also destroy everything at hand and put ourselves at risk. We overestimated the problem, and our solution wasn't good. This is overfitting: our data is simple, but we try to fit it to a model that is too complex. The model will be able to fit our data, but it'll memorize it instead of learning it. The first time I learned overfitting, my reaction was, \"Well, that's no problem. If I use a model that is too complex, I can still model my data, right?\" Correct, but the real problem with overfitting is trying to get the model to make predictions on unseen data. The predictions will likely come out looking horrible, as we see later in this chapter.",
    "source": "chapter_81_4 Optimizing the training process_ Underfitting_ overfitting_ testing_ and regularization.pdf::chunk3"
  },
  {
    "text": "103 In this chapter • what is classification • sentiment analysis: how to tell if a sentence is happy or sad using machine learning • how to draw a line that separates points of two colors • what is a perceptron, and how do we train it • coding the perceptron algorithm in Python and Turi CreateUsing lines to split our points: The perceptron algorithm 5 then my friends mary and jane also found the pattern, but my friends john and carl didn't.my aunt julia found the pattern. but my uncle bob didn't. my mom found it. but my dad didn't. can you guys see the pattern? nope.not a clue. what is she talking about? a cla cla cl what 104 Chapter 5 I Using lines to split our points: The perceptron algorithm In this chapter, we learn a branch of machine learning called classification . Classification models are similar to regression models, in that their aim is to predict the labels of a dataset based on the features. The difference is that regression models aim to predict a number, whereas classification models aim to predict a state or a category. Classification models are often called classifiers, and we'll use",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk0"
  },
  {
    "text": "the terms interchangeably. Many classifiers predict one of two possible states (often yes/no), although it is possible to build classifiers that predict among a higher number of possible states. The following are popular examples of classifiers: • A recommendation model that predicts whether a user will watch a certain movie • An email model that predicts whether an email is spam or ham • A medical model that predicts whether a patient is sick or healthy • An image-recognition model that predicts whether an image contains an automobile, a bird, a cat, or a dog • A voice recognition model that predicts whether the user said a particular command Classification is a popular area in machine learning, and the bulk of the chapters in this book (chapters 5, 6, 8, 9, 10, 11, and 12) talk about different classification models. In this chapter, we learn the perceptron model, also called the perceptron classifier , or simply the perceptron . A perceptron is similar to a linear regression model, in that it uses a linear combination of the features to make a prediction and is the building block of neural networks (which we learn in chapter 10). Furthermore, the process of",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk1"
  },
  {
    "text": "training a perceptron is similar to that of training a linear regression model. Just as we did in chapter 3 with the linear regression algorithm, we develop the perceptron algorithm in two ways: using a trick that we can iterate many times, and defining an error function that we can minimize using gradient descent. The main example of classification models that we learn in this chapter is sentiment analysis . In sentiment analysis, the goal of the model is to predict the sentiment of a sentence. In other words, the model predicts whether the sentence is happy or sad. For example, a good sentiment analysis model can predict that the sentence \"I feel wonderful!\" is a happy sentence, and that the sentence \"What an awful day!\" is a sad sentence. Sentiment analysis is used in many practical applications, such as the following: • When a company analyzes the conversations between customers and technical support, to evaluate the quality of the conversation • When analyzing the tone of a brand's digital presence, such as comments on social media or reviews related to its products • When a social platform like Twitter analyzes the overall mood of a certain population after an",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk2"
  },
  {
    "text": "event • When an investor uses public sentiment toward a company to predict its stock price How could we build a sentiment analysis classifier? In other words, how could we build a machine learning model that takes a sentence as an input and, as output, tells us whether the sentence is Using lines to split our points: The perceptron algorithm 105 happy or sad. This model can make mistakes, of course, but the idea is to build it in such a way that it makes as few mistakes as possible. Let's put down the book for a couple of minutes and think of how we would go about building this type of model. Here is an idea. Happy sentences tend to contain happy words, such as wonderful , happy , or joy, whereas sad sentences tend to contain sad words, such as awful , sad, or despair . A classifier can consist of a \"happiness\" score for every single word in the dictionary. Happy words can be given positive scores, and sad words can be given negative scores. Neutral words such as the can be given a score of zero. When we feed a sentence into our classifier, the classifier",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk3"
  },
  {
    "text": "simply adds the scores of all the words in the sentence. If the result is positive, then the classifier concludes that the sentence is happy. If the result is negative, then the classifier concludes that the sentence is sad. The goal now is to find scores for all the words in the dictionary. For this, we use machine learning. The type of model we just built is called a perceptron model . In this chapter, we learn the formal definition of a perceptron and how to train it by finding the perfect scores for all the words so that our classifier makes as few mistakes as possible. The process of training a perceptron is called the perceptron algorithm , and it is not that different from the linear regression algorithm we learned in chapter 3. Here is the idea of the perceptron algorithm: To train the model, we first need a dataset containing many sentences together with their labels (happy/sad). We start building our classifier by assigning random scores to all the words. Then we go over all the sentences in our dataset several times. For every sentence, we slightly tweak the scores so that the classifier improves the prediction",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk4"
  },
  {
    "text": "for that sentence. How do we tweak the scores? We do it using a trick called the perceptron trick , which we learn in the section \"The perception trick.\" An equivalent way to train perceptron models is to use an error function, just as we did in chapter 3. We then use gradient descent to minimize this function. However, language is complicated—it has nuances, double entendres, and sarcasm. Wouldn't we lose too much information if we reduce a word to a simple score? The answer is yes—we do lose a lot of information, and we won't be able to create a perfect classifier this way. The good news is that using this method, we can still create a classifier that is correct most of the time. Here is a proof that the method we are using can't be correct all the time. The sentences, \"I am not sad, I'm happy\" and \"I am not happy, I am sad\" have the same words, yet completely different meanings. Therefore, no matter what scores we give the words, these two sentences will attain the exact same score, and, thus, the classifier will return the same prediction for them. They have different labels, so",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk5"
  },
  {
    "text": "the classifier must have made a mistake with one of them. A solution for this problem is to build a classifier that takes the order of the words into account, or even other things such as punctuation or idioms. Some models such as hidden Markov models (HMM) , recurrent neural networks (RNN) , or long short-term memory networks (LSTM) have had great success with sequential data, but we won't include them in this book. However, if you want to explore these models, in appendix C you can find some very useful references for that. You can find all the code for this chapter in the following GitHub repository: https://github .com/luisguiserrano/manning/tree/master/Chapter_5_Perceptron_Algorithm.",
    "source": "chapter_98_5 Using lines to split our points_ The perceptron algorithm.pdf::chunk6"
  }
]